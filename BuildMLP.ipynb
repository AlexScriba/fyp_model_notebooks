{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building MLP Model\n",
    "This Notebook contains the minimum code needed to build, train and save the MLP model.\n",
    "\n",
    "It starts with setting up required logic and pre-processing the data. Then builds and trains the model. Lastly the necessary models are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATA_POINTS = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_PATH = './Accelerometer/'\n",
    "GRAV_PATH = './Gravity/'\n",
    "GYRO_PATH = './Gyroscope/'\n",
    "ROT_PATH = './Rotation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n"
     ]
    }
   ],
   "source": [
    "file_names = os.listdir(ACC_PATH)\n",
    "print(len(file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileName(file):\n",
    "    return file.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    return list(map(lambda x: float(x), row))\n",
    "\n",
    "def getData(file_path):\n",
    "    res = []\n",
    "\n",
    "    with open(file_path) as csvfile:\n",
    "        r = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in r:\n",
    "            res.append(row)\n",
    "\n",
    "    res = list(map(parse, res[1:]))\n",
    "    \n",
    "    for row in res:\n",
    "        row[-1] = int(row[-1])\n",
    " \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(data, numAfter):\n",
    "    x = np.array(list(map(lambda x: x[-1], data)))\n",
    "\n",
    "    new_x = np.linspace(x.min(), x.max(), numAfter)\n",
    "\n",
    "    res = []\n",
    "\t\n",
    "    for i in range(len(data[0]) - 1) :\n",
    "        y = list(map(lambda x: x[i], data))\n",
    "\n",
    "        new_y = sp.interpolate.interp1d(x, y, kind='cubic')(new_x)\n",
    "        res.append(new_y)\n",
    "        # np.append(res, [new_y])\n",
    "\n",
    "    return np.array(res).transpose(1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineFile(file_name):\n",
    "\tname = fileName(file_name)\n",
    "\n",
    "\taccData = getData(ACC_PATH + file_name)\n",
    "\tgravData = getData(GRAV_PATH+ file_name)\n",
    "\tgyroData = getData(GYRO_PATH + file_name)\n",
    "\trotData = getData(ROT_PATH + file_name)\n",
    "\n",
    "\taccData = interpolate(accData, NUM_DATA_POINTS)\n",
    "\tgravData = interpolate(gravData, NUM_DATA_POINTS)\n",
    "\tgyroData = interpolate(gyroData, NUM_DATA_POINTS)\n",
    "\trotData = interpolate(rotData, NUM_DATA_POINTS)\n",
    "\n",
    "\taccumData = []\n",
    "\n",
    "\tfor i in range(len(accData)):\n",
    "\t\taccumData.append(np.concatenate([accData[i], gyroData[i], gravData[i], rotData[i]]))\n",
    "\n",
    "\treturn np.array(accumData)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combineFile('a_1_0.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchData(names):\n",
    "    retDict = {}\n",
    "\n",
    "    for file_name in names:\n",
    "        name = fileName(file_name)\n",
    "        data = combineFile(file_name)\n",
    "        \n",
    "        try:\n",
    "            retDict[name].append(data)\n",
    "        except:\n",
    "            retDict[name] = [data]\n",
    "    \n",
    "    return retDict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = fetchData(os.listdir(ACC_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupData(dic, user, attacker):\n",
    "    user_data = np.array(dic[user])\n",
    "    attacker_data = np.concatenate(list(map(lambda x: dic[x], attacker)))\n",
    "\n",
    "    user_labels = [1] * len(user_data)\n",
    "    attacker_labels = [0] * len(attacker_data)\n",
    "\n",
    "    all_labels = np.concatenate((user_labels, attacker_labels))\n",
    "    all_data = np.concatenate([user_data, attacker_data])\n",
    "\n",
    "    return (all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupDataAttackers(dic, attacker):\n",
    "    attacker_data = np.concatenate(list(map(lambda x: dic[x], attacker)))\n",
    "    attacker_labels = [0] * len(attacker_data)\n",
    "\n",
    "    return (attacker_data,  attacker_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupData_ident(dic, allpeople):\n",
    "    all_data = np.concatenate(list(map(lambda x: dic[x], allpeople)))\n",
    "    all_labels=[]\n",
    "    \n",
    "    i=0\n",
    "    for peeps in dic.keys():\n",
    "        all_labels = np.concatenate((all_labels, len(dict[peeps])*[i]))\n",
    "        i=i+1\n",
    "\n",
    "    return (all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zv/2swtbyvx6cn5m4n_0p8hf4780000gn/T/ipykernel_89159/3950229790.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.core.display.set_matplotlib_formats(\"svg\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import rfft, irfft, fftfreq, fft, ifft\n",
    "import copy\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpbf(y):\n",
    "    sos = signal.butter(10, 7, 'low', fs=1000, output='sos')\n",
    "    filtered = signal.sosfilt(sos, y)\n",
    "    #filtered=filtered[:, 200:6800]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {k: list(map(lambda a: lpbf(a.transpose(1,0)).transpose(1,0), v)) for k, v in dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(100)\n",
    "import csv\n",
    "from scipy import io\n",
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_transform(model, mfccs):\n",
    "    numwords = model.cluster_centers_.shape[0]\n",
    "    bows = zeros((len(mfccs), numwords))\n",
    "    for i in range(len(mfccs)):\n",
    "        w = model.predict(mfccs[i])\n",
    "        bw = bincount(w, minlength=numwords)\n",
    "        bows[i,:] = bw\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1237, 7000, 12)\n",
      "(1237,)\n"
     ]
    }
   ],
   "source": [
    "target = 'dd'\n",
    "attackers = [x for x in dict.keys() if x != target]\n",
    "\n",
    "(allData, allLabels) = setupData(dict, target, attackers)\n",
    "\n",
    "print(allData.shape)\n",
    "print(allLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 80)\n",
      "a :  0.9435483870967742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10483870967741936"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine=0\n",
    "# for target in dict.keys():\n",
    "for target in ['a']:\n",
    "    attackers = [x for x in dict.keys() if x != target]\n",
    "    (allData, allLabels) = setupData(d2, target, attackers)\n",
    "    #allData=allData[:,:,-3:6]\n",
    "    #np.delete(allData, np.s_[6:9], axis=1) \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        allData,\n",
    "        allLabels,\n",
    "        random_state=101,\n",
    "        test_size = 0.3,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_data=array(x_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = train_data.shape\n",
    "    train_data = np.reshape(train_data, newshape=(-1, num_features))\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "    x_train = np.reshape(train_data, newshape=(num_instances, num_time_steps, num_features))\n",
    "\n",
    "    val_data=array(x_test)\n",
    "\n",
    "    num_instances, num_time_steps, num_features = val_data.shape\n",
    "    val_data = np.reshape(val_data, newshape=(-1, num_features))\n",
    "    val_data = scaler.transform(val_data)\n",
    "\n",
    "    x_test = np.reshape(val_data, newshape=(num_instances, num_time_steps, num_features)) \n",
    "    \n",
    "    all_dmfccs = vstack(x_train)\n",
    "    km = cluster.MiniBatchKMeans(n_clusters=80, random_state=5489, n_init=10, batch_size = 2048, verbose=0)\n",
    "    km.fit(all_dmfccs[0::10])  # subsample by 10 to make it faster\n",
    "    km.cluster_centers_\n",
    "    train_bow = bow_transform(km, x_train)\n",
    "    test_bow  = bow_transform(km, x_test)\n",
    "\n",
    "    #feature extraction using BOAW\n",
    "    tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
    "    train_Xtf = tf_trans.fit_transform(train_bow)\n",
    "    test_Xtf  = tf_trans.transform(test_bow)\n",
    "\n",
    "    print(test_Xtf.shape)\n",
    "    \n",
    "    #print(train_Xtf.shape)\n",
    "    \n",
    "    paramgrid = {'C': logspace(-2,3,20), \n",
    "             'gamma': logspace(-4,3,20) }\n",
    "\n",
    "    mlp = MLPClassifier(solver='adam', max_iter=5000, random_state=0, hidden_layer_sizes=[200, 200])\n",
    "    mlp.fit(train_Xtf, y_train)\n",
    "\n",
    "    predY = mlp.predict(test_Xtf)\n",
    "    acc = metrics.accuracy_score(y_test, predY)\n",
    "    print(target, \": \", acc)\n",
    "    combine=combine+acc\n",
    "    \n",
    "combine/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cluster._kmeans.MiniBatchKMeans"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(km)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save(model, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(mlp, 'mlp_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(km, 'km_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(tf_trans, 'tf_transformer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
